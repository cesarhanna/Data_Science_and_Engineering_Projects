{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7dc5ab01-f930-45d7-aba9-1f4a37095114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the encessary packages:\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d06965ae-cbbc-48cc-834f-660d529fe337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establishing the connection to the Database:\n",
    "conn = psycopg2.connect(host='127.0.0.1',\n",
    "        port=5432,\n",
    "        database='dbcesar',\n",
    "        user='postgres',\n",
    "        password='CesarHanna',\n",
    "        connect_timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ba34bf6-7ee7-413f-929d-30870e1bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the engine that will be responsible to execute sql queries in the Database:\n",
    "engine = create_engine('postgresql://postgres:CesarHanna@127.0.0.1:5432/dbcesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a25e0635-6a2a-43d0-bd04-83a858a65e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_storing_stream (p_num):\n",
    "    #Creating the necessary lists where the data scraped/fetched will reside:\n",
    "    Date = []\n",
    "    Time = []\n",
    "    Temperature = []\n",
    "    ReelFeel_Temperature = []\n",
    "    ReelFeel_Temperature_Shade = []\n",
    "    Max_UV_Index = []\n",
    "    Wind = []\n",
    "    Probability_of_Thunderstorms = []\n",
    "    Cloud_Cover = []\n",
    "    \n",
    "    #Requesting the webpage by using the webdriver:\n",
    "    webpage = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    webpage.get('https://www.accuweather.com/en/ae/dubai/323091/daily-weather-forecast/323091?day=' + str(p_num))\n",
    "    \n",
    "    #Getting the content:\n",
    "    content = webpage.page_source\n",
    "    #Parsing the content result as html using BeatifulSoup:\n",
    "    result = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    #Fetching the section that we will scrape from:\n",
    "    weather_data = result.find_all('div', {'class': 'page-content content-module'})\n",
    "    \n",
    "    #Looping through the weather_data and appending the Time and Temperature lists:\n",
    "    for item in weather_data:\n",
    "        for time in item.find_all('div', {'class': 'half-day-card-header__title'}):\n",
    "            Time.append(time.find('h2', {'class': 'title'}).string)\n",
    "        for temp in item.find_all('div', {'class': 'weather'}):\n",
    "            Temperature.append(temp.find('div', {'class': 'temperature'}).text.replace('\\n', '').replace('\\t', ''))\n",
    "            \n",
    "    #Based on the complexity of retrieving data using HTML, I am using XPATH which is a simpler approach:\n",
    "    Date.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[1]/div').text) # date matching the day\n",
    "    Date.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[1]/div').text) #date matching the night\n",
    "\n",
    "    Max_UV_Index.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/p[1]/span').text)\n",
    "    Max_UV_Index.append('') #appending null since this data is not relevant at night\n",
    "\n",
    "    Wind.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span').text) #during day\n",
    "    Wind.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[4]/div[2]/div[2]/div[1]/p[1]/span').text) #during night\n",
    "\n",
    "    Probability_of_Thunderstorms.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[2]/div[2]/div[2]/p[1]/span').text) #during day\n",
    "    Probability_of_Thunderstorms.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[4]/div[2]/div[2]/div[2]/p[1]/span').text) #during night\n",
    "\n",
    "    Cloud_Cover.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[2]/div[2]/div[2]/p[3]/span').text) #during day\n",
    "    Cloud_Cover.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[4]/div[2]/div[2]/div[2]/p[3]/span').text) #during night\n",
    "\n",
    "    ReelFeel_Temperature.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[1]/div[2]/div[2]/div[1]').text) #during day\n",
    "    ReelFeel_Temperature.append(webpage.find_element(By.XPATH,'/html/body/div/div[7]/div[1]/div[1]/div[4]/div[1]/div[2]/div[2]/div').text) #during night\n",
    "\n",
    "    ReelFeel_Temperature_Shade.append(webpage.find_element(By.XPATH, '/html/body/div/div[7]/div[1]/div[1]/div[2]/div[1]/div[2]/div[2]/div[2]/div').text)\n",
    "    ReelFeel_Temperature_Shade.append('') #appending null since this data is not relevant at night\n",
    "    \n",
    "    #Cleaning the ReelFeel_Temperature list by keeping only the temperature measure:\n",
    "    ReelFeel = []\n",
    "    for i in ReelFeel_Temperature:\n",
    "        ReelFeel.append(i.split(' ')[-1])\n",
    "        \n",
    "    #Cleaning the ReelFeel_Temperature_Shade list by keeping only the temperature measure:\n",
    "    ReelFeel_Shade = []\n",
    "    for i in ReelFeel_Temperature_Shade:\n",
    "        ReelFeel_Shade.append(i.split(' ')[-1])\n",
    "        \n",
    "    #Splitting the Max_UV_Index list and creating an additional list that holds the severity of the UV index:\n",
    "    UV_Index = []\n",
    "    UV_Index_Severity = []\n",
    "    for i in Max_UV_Index:\n",
    "        UV_Index.append(i.split(' ')[0])\n",
    "        UV_Index_Severity.append(i[2:].lstrip(' '))\n",
    "        \n",
    "    #Creating a dataframe out of the lists:\n",
    "    weather_df_day = pd.DataFrame({'Date': Date, 'Time': Time, 'Temperature': Temperature, 'ReelFeel': ReelFeel, 'ReelFeel_Shade': ReelFeel_Shade, 'UV_Index': UV_Index, \\\n",
    "                                 'UV_Index_Severity': UV_Index_Severity, 'Wind': Wind, 'Probability_of_Thunderstorms': Probability_of_Thunderstorms, 'Cloud_Cover': Cloud_Cover})\n",
    "    \n",
    "    #Converting the Dataframe into CSV Format and Storing it into the Local Machine:\n",
    "    weather_df_day.to_csv(r\"C:\\Users\\cesar\\OneDrive\\Documents\\Cesar documents\\Data Science Projects\\Weather Data Streaming Project\\Streaming Datasets - CSV\\weather_day{}.csv\".format(p_num), index=False)\n",
    "    \n",
    "    #Writing the datafrmae/file into Postgres using autocommit:\n",
    "    copy_activity ='''COPY public.daily_weather_streaming (\"Date\", \"Time\", \"Temperature\", \"ReelFeel\", \"ReelFeel_Shade\", \"UV_Index\", \"UV_Index_Severity\", \"Wind\", \"Probability_of_Thunderstorms\", \"Cloud_Cover\") \\\n",
    "                     FROM 'C:\\\\Users\\\\cesar\\\\OneDrive\\\\Documents\\\\Cesar documents\\\\Data Science Projects\\\\Weather Data Streaming Project\\\\Streaming Datasets - CSV\\\\weather_day{}.csv' WITH (FORMAT CSV, HEADER);'''.format(p_num)\n",
    "    write_df = engine.execute(text(copy_activity).execution_options(autocommit=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b686c287-a2f2-4297-8c81-82dc4e8e6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigerring the stream, where each iteration represents 1 day:\n",
    "for i in range(1, 7):\n",
    "   scraping_storing_stream(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c14ad8-9d1f-4a53-ac72-75a956ac928a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
